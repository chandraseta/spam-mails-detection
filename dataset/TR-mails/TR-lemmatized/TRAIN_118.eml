question raid disclaimer partial xfs tim clewlow put forth reticence use ext xfs due long cache write time claim dangerous event kernel lockup power outage problem linux buffer cache implementation one filesystem problem code fact trade performance data integrity journaling filesystem prevent loss data linux buffer cache machine crash zero delete file fully write crash order keep fs consistent state always lose data flight fs get corrupt due journal replay reboot seriously concern loss write data buffer cache system crash mount filesystems sync fstab options write get flush disk without queue buffer cache also report albeit perhaps somewhat date ext xfs still small important bug iron would happy hear people experience demonstrate longer true preference would ext instead xfs believe opinion likely become successor ext future speak well ext xfs fully production quality many years since irix introduce since linux bug identify result fs inconsistency crash fix bug fix work since deal minor issue unrelated data integrity code fix work quite time cleanup work optimizations write better documentation read post xfs mail list informative quality performance code xfs really sharp devs current former sgi engineer want know ext handle tb fs know delay allocation write turn ext among tune options look ext fs size longer question really hop ext way go xfs even tune options ext pretty much every fs matter xfs bite kernel max fs file size tb bite kernel exabytes xfs better solution ext point ted t so admit last week one function call ext terrible shape lot work fix todo list fix ext call write_cache_pages seriously abuse function atm since actually write page call write_cache_pages go embarassing suffice say end call pagevec_lookup pagevec_lookup_tag four count four time try writeback simple patch give ext copy write_cache_pages simplify lot fix bunch problems discard favor fundamentally redo writeback go take get things completely right work try fix also hop cpu motherboard suitable grunt fsb bandwidth could reduce performance problems software raid seriously mistake would love know beforehand reticence use hw raid seem like add one point possible failure could easily paranoid dismiss reason good hardware raid card really nice give feature really get md raid true yank drive tray hot swap capability try read md raid like yank active drive fault lead drive audible warn also nice hw raid solutions main advantage performance decent hw raid almost always faster md raid sometimes factor depend disk count raid level typically good hw raid really trounce md raid performance level basically anything require parity calculations sound like casual user need lot protect disk space necessarily absolute blaze speed linux raid fine take closer look xfs make decision fs array get whole lot like feature exactly tune xfs mdadm raid setup fact usually automatically do mkfs xfs query block device device driver stride width info match man mkfs xfs http http http http http note date note praise hans reiser lavish upon xfs http node_id http http rc benchmarks filesystems tree xfs users linux kernel archive bite year ago october kernel org ever increase need squeeze performance machine make leap migrate primary mirror machine mirrors kernel org xfs site number reason include fscking t disk long painful hit various cache issue seek better performance file system initial test look positive make jump quite happy result instant increase performance throughput well worst xfs_check ever see take minutes quite happy subsequently move primary mirror file systems xfs include www kernel org mirrors kernel org average constant movement mbps around world peak gbps range serve thousands users simultaneously file system take brunt throw hold spectacularly stan unsubscribe email debian user request lists debian org subject unsubscribe trouble contact listmaster lists debian org archive http hardwarefreak com